/**
 * Enhanced Voice Analysis Integration for Interview Feedback System
 * 
 * This module extends the existing text-based feedback system to include:
 * - Audio transcription and analysis
 * - Speech quality assessment (pace, clarity, confidence)
 * - Pronunciation feedback
 * - Multi-modal session analysis combining text content and voice delivery
 */

import { GoogleGenAI } from '@google/genai';
import type { 
  JdResumeText, 
  Persona, 
  MvpSessionTurn, 
  SessionFeedbackData,
  ConversationalResponse 
} from '../types';

// üéØ ENHANCED TYPES FOR VOICE ANALYSIS

interface VoiceMetrics {
  avgPace: number; // words per minute
  pauseDuration: number; // average pause length in seconds
  voiceConfidence: number; // 0-100 confidence score
  filler_words: number; // count of "um", "uh", etc.
  volume_consistency: number; // 0-100 consistency score
  pronunciation_issues: string[]; // specific words that need work
}

interface AudioAnalysisResult {
  transcription: string;
  voiceMetrics: VoiceMetrics;
  speechQuality: 'excellent' | 'good' | 'needs_improvement' | 'poor';
  keyInsights: string[];
  recommendations: string[];
}

interface EnhancedSessionFeedback extends SessionFeedbackData {
  // Additional voice-specific feedback
  speechAnalysis?: {
    overallDelivery: number; // 0-100 score
    pronunciationScore: number;
    paceAndFlow: number;
    confidenceLevel: number;
    recommendations: string[];
  };
  audioInsights?: string[];
  pronunciationFeedback?: {
    problemWords: { word: string; suggestion: string }[];
    overallClarity: number;
    accentNotes?: string;
  };
}

// üé§ VOICE CONVERSATION WITH ANALYSIS

export interface VoiceConversationResponse extends ConversationalResponse {
  audioResponse?: Blob; // TTS audio for AI response
  speechAnalysis?: AudioAnalysisResult; // Analysis of user's voice input
  deliveryFeedback?: string[]; // Immediate feedback on speech delivery
}

// üîß ENHANCED GEMINI CLIENT WITH VOICE CAPABILITIES

const GEMINI_API_KEY = process.env.GEMINI_API_KEY ?? '';
const isTestEnvironment = process.env.NODE_ENV === 'test' || process.env.JEST_WORKER_ID !== undefined;

if (!GEMINI_API_KEY && !isTestEnvironment) {
  throw new Error("CRITICAL: GEMINI_API_KEY is not set.");
}

const genAI = new GoogleGenAI({
  apiKey: GEMINI_API_KEY || 'test-key-for-mocking',
});

// Voice-capable models
const VOICE_MODEL = 'gemini-2.5-flash-preview-native-audio-dialog';
const TEXT_TO_SPEECH_MODEL = 'gemini-2.5-flash-tts';
const ANALYSIS_MODEL = 'gemini-2.0-flash-001'; // Keep text model for analysis

// üéØ CORE VOICE CONVERSATION FUNCTION

/**
 * Enhanced conversation function that handles both text and voice inputs
 * Provides voice analysis alongside conversational responses
 */
export async function continueConversationWithVoice(
  jdResumeText: JdResumeText,
  persona: Persona,
  history: MvpSessionTurn[],
  userInput: string | Blob, // Can be text or audio
  options: {
    outputModality?: 'text' | 'voice' | 'both';
    analyzeVoice?: boolean;
    voiceSettings?: {
      voice: 'Aoede' | 'Charon' | 'Fenrir' | 'Kore' | 'Puck';
      speed?: number;
      language?: string;
    };
  } = {}
): Promise<VoiceConversationResponse> {
  const { outputModality = 'text', analyzeVoice = true, voiceSettings } = options;

  try {
    let transcribedText = '';
    let speechAnalysis: AudioAnalysisResult | undefined;

    // üìù STEP 1: Handle voice input (transcription + analysis)
    if (userInput instanceof Blob) {
      console.log('üé§ Processing voice input...');
      
      // Transcribe audio using Gemini
      const transcriptionResult = await transcribeAudio(userInput);
      transcribedText = transcriptionResult.transcription;
      
      if (analyzeVoice) {
        speechAnalysis = await analyzeVoiceQuality(userInput, transcribedText);
      }
    } else {
      transcribedText = userInput;
    }

    // üìä STEP 2: Generate conversational response using text model
    const textResponse = await generateTextResponse(
      jdResumeText,
      persona,
      history,
      transcribedText
    );

    // üîä STEP 3: Generate voice output if requested
    let audioResponse: Blob | undefined;
    if (outputModality === 'voice' || outputModality === 'both') {
      audioResponse = await generateVoiceResponse(
        textResponse.followUpQuestion,
        voiceSettings
      );
    }

    // üí° STEP 4: Add voice-specific feedback
    const deliveryFeedback = speechAnalysis 
      ? generateDeliveryFeedback(speechAnalysis)
      : undefined;

    return {
      ...textResponse,
      audioResponse,
      speechAnalysis,
      deliveryFeedback,
    };

  } catch (error) {
    console.error('üö® Voice conversation error:', error);
    
    // Fallback to text-only
    const fallbackText = userInput instanceof Blob ? '[Voice input failed to process]' : userInput;
    const fallbackResponse = await generateTextResponse(jdResumeText, persona, history, fallbackText);
    
    return {
      ...fallbackResponse,
      deliveryFeedback: ['Voice processing temporarily unavailable, continuing with text analysis'],
    };
  }
}

// üéôÔ∏è AUDIO TRANSCRIPTION

async function transcribeAudio(audioBlob: Blob): Promise<{ transcription: string }> {
  try {
    // Convert blob to base64 for Gemini API
    const arrayBuffer = await audioBlob.arrayBuffer();
    const base64Audio = btoa(String.fromCharCode(...new Uint8Array(arrayBuffer)));

    const response = await genAI.models.generateContent({
      model: VOICE_MODEL,
      contents: [{
        role: 'user',
        parts: [
          {
            text: 'Please transcribe this audio accurately, maintaining natural speech patterns and hesitations.'
          },
          {
            inlineData: {
              mimeType: 'audio/wav', // Adjust based on actual audio format
              data: base64Audio
            }
          }
        ]
      }]
    });

    const transcription = response.response.text() || '[Transcription failed]';
    
    return { transcription };

  } catch (error) {
    console.error('üö® Transcription error:', error);
    return { transcription: '[Voice transcription failed - please try again]' };
  }
}

// üìä VOICE QUALITY ANALYSIS

async function analyzeVoiceQuality(audioBlob: Blob, transcription: string): Promise<AudioAnalysisResult> {
  try {
    const arrayBuffer = await audioBlob.arrayBuffer();
    const base64Audio = btoa(String.fromCharCode(...new Uint8Array(arrayBuffer)));

    const analysisPrompt = `
      Analyze this interview response audio for speech quality and delivery:
      
      Transcription: "${transcription}"
      
      Please provide analysis in this JSON format:
      {
        "voiceMetrics": {
          "avgPace": <words per minute>,
          "pauseDuration": <average pause in seconds>,
          "voiceConfidence": <0-100 confidence score>,
          "filler_words": <count of um, uh, etc>,
          "volume_consistency": <0-100 consistency score>,
          "pronunciation_issues": ["word1", "word2"]
        },
        "speechQuality": "<excellent|good|needs_improvement|poor>",
        "keyInsights": ["insight1", "insight2", "insight3"],
        "recommendations": ["recommendation1", "recommendation2"]
      }
    `;

    const response = await genAI.models.generateContent({
      model: VOICE_MODEL,
      contents: [{
        role: 'user',
        parts: [
          { text: analysisPrompt },
          {
            inlineData: {
              mimeType: 'audio/wav',
              data: base64Audio
            }
          }
        ]
      }]
    });

    const analysisText = response.response.text();
    
    try {
      const analysis = JSON.parse(analysisText) as {
        voiceMetrics: VoiceMetrics;
        speechQuality: 'excellent' | 'good' | 'needs_improvement' | 'poor';
        keyInsights: string[];
        recommendations: string[];
      };

      return {
        transcription,
        ...analysis
      };

    } catch (parseError) {
      console.error('üö® Failed to parse voice analysis JSON:', parseError);
      return createFallbackVoiceAnalysis(transcription);
    }

  } catch (error) {
    console.error('üö® Voice analysis error:', error);
    return createFallbackVoiceAnalysis(transcription);
  }
}

function createFallbackVoiceAnalysis(transcription: string): AudioAnalysisResult {
  // Simple heuristic analysis when AI analysis fails
  const wordCount = transcription.split(' ').length;
  const fillerWords = (transcription.match(/\b(um|uh|er|ah|like|you know)\b/gi) || []).length;
  
  return {
    transcription,
    voiceMetrics: {
      avgPace: Math.max(100, Math.min(200, wordCount * 2)), // Rough estimate
      pauseDuration: 0.5,
      voiceConfidence: Math.max(30, 80 - fillerWords * 10),
      filler_words: fillerWords,
      volume_consistency: 75,
      pronunciation_issues: []
    },
    speechQuality: fillerWords > 3 ? 'needs_improvement' : 'good',
    keyInsights: [
      'Voice analysis completed with basic metrics',
      `Detected ${fillerWords} filler words`,
      'Consider practicing smoother delivery'
    ],
    recommendations: [
      'Practice speaking more slowly and clearly',
      'Reduce use of filler words like "um" and "uh"',
      'Record yourself practicing to improve delivery'
    ]
  };
}

// üí¨ TEXT RESPONSE GENERATION (existing logic)

async function generateTextResponse(
  jdResumeText: JdResumeText,
  persona: Persona,
  history: MvpSessionTurn[],
  userResponse: string
): Promise<ConversationalResponse> {
  // Use existing continueConversation logic - import from gemini.ts
  // This maintains consistency with your current text-based system
  
  const recentHistory = history.slice(-6);
  const conversationSoFar = recentHistory
    .map(turn => `${turn.role === 'user' ? 'Candidate' : 'Interviewer'}: ${turn.text}`)
    .join('\n');

  const prompt = `
    You are a ${persona.name} having a natural interview conversation.
    
    Recent conversation:
    ${conversationSoFar}

    Candidate just said: "${userResponse}"

    INSTRUCTIONS:
    - Respond naturally based on what they just shared
    - Ask thoughtful follow-up questions about the same topic
    - Show genuine curiosity about their experience
    - Keep responses concise (1-2 sentences max)
  `;

  const response = await genAI.models.generateContentStream({
    model: ANALYSIS_MODEL,
    contents: [{ role: 'user', parts: [{ text: prompt }] }],
    config: {
      temperature: 0.8,
      maxOutputTokens: 400,
      topP: 0.9,
    },
  });

  let rawAiResponseText = '';
  for await (const chunk of response.stream) {
    rawAiResponseText += chunk.text();
  }

  const insights = extractInsightsFromResponse(userResponse);

  return {
    analysis: `Your response shows engagement with the topic. Let's explore this further.`,
    feedbackPoints: insights,
    followUpQuestion: rawAiResponseText.trim(),
    rawAiResponseText,
  };
}

// üîä TEXT-TO-SPEECH GENERATION

async function generateVoiceResponse(
  text: string,
  voiceSettings?: {
    voice: 'Aoede' | 'Charon' | 'Fenrir' | 'Kore' | 'Puck';
    speed?: number;
    language?: string;
  }
): Promise<Blob> {
  try {
    const voice = voiceSettings?.voice || 'Aoede';
    const speed = voiceSettings?.speed || 1.0;
    
    const response = await genAI.models.generateContent({
      model: TEXT_TO_SPEECH_MODEL,
      contents: [{
        role: 'user',
        parts: [{ 
          text: `Generate speech for: "${text}"` 
        }]
      }],
      config: {
        voice: voice,
        speechRate: speed,
        outputFormat: 'WAV'
      }
    });

    // Convert response to audio blob
    // Note: This is pseudocode - actual implementation depends on Gemini TTS API format
    const audioData = response.response.audioData; // Gemini-specific field
    return new Blob([audioData], { type: 'audio/wav' });

  } catch (error) {
    console.error('üö® TTS generation error:', error);
    // Return empty blob as fallback
    return new Blob([''], { type: 'audio/wav' });
  }
}

// üí° DELIVERY FEEDBACK GENERATION

function generateDeliveryFeedback(analysis: AudioAnalysisResult): string[] {
  const feedback: string[] = [];

  // Pace feedback
  if (analysis.voiceMetrics.avgPace < 120) {
    feedback.push("Consider speaking a bit faster to maintain engagement");
  } else if (analysis.voiceMetrics.avgPace > 180) {
    feedback.push("Try slowing down slightly for better clarity");
  } else {
    feedback.push("Good speaking pace - clear and engaging");
  }

  // Confidence feedback
  if (analysis.voiceMetrics.voiceConfidence < 60) {
    feedback.push("Project more confidence in your voice - stand tall and speak with conviction");
  } else if (analysis.voiceMetrics.voiceConfidence > 80) {
    feedback.push("Excellent confident delivery!");
  }

  // Filler words feedback
  if (analysis.voiceMetrics.filler_words > 3) {
    feedback.push(`Reduce filler words ("um", "uh") - you used ${analysis.voiceMetrics.filler_words} in this response`);
  } else if (analysis.voiceMetrics.filler_words === 0) {
    feedback.push("Great job avoiding filler words - very professional delivery");
  }

  // Pronunciation feedback
  if (analysis.voiceMetrics.pronunciation_issues.length > 0) {
    feedback.push(`Practice pronunciation of: ${analysis.voiceMetrics.pronunciation_issues.join(', ')}`);
  }

  return feedback;
}

// üìä ENHANCED SESSION ANALYSIS FOR VOICE

/**
 * Analyzes complete session including both content and voice delivery
 * This integrates with your existing getSessionFeedback system
 */
export async function generateEnhancedSessionFeedback(
  sessionId: string,
  history: MvpSessionTurn[],
  jdResumeText: JdResumeText,
  includeVoiceAnalysis: boolean = false
): Promise<EnhancedSessionFeedback> {
  try {
    // Extract all user responses for analysis
    const userResponses = history.filter(turn => turn.role === 'user');
    const conversationContent = history.map(turn => 
      `${turn.role === 'user' ? 'Candidate' : 'Interviewer'}: ${turn.text}`
    ).join('\n');

    // Generate comprehensive feedback prompt
    const feedbackPrompt = `
      Analyze this complete interview session and provide detailed feedback:

      Job Description Context:
      ${jdResumeText.jdText}

      Resume Context:
      ${jdResumeText.resumeText}

      Full Conversation:
      ${conversationContent}

      Please provide analysis in this JSON format:
      {
        "overallScore": <0-100 score>,
        "strengths": ["strength1", "strength2", "strength3"],
        "areasForImprovement": ["area1", "area2", "area3"],
        "recommendations": ["rec1", "rec2", "rec3"],
        "detailedAnalysis": "Comprehensive analysis paragraph",
        "skillAssessment": {
          "Communication": <0-100>,
          "Technical Knowledge": <0-100>,
          "Problem Solving": <0-100>,
          "Leadership": <0-100>,
          "Adaptability": <0-100>,
          "Teamwork": <0-100>
        }
      }

      Focus on specific examples from their responses and provide actionable feedback.
    `;

    const response = await genAI.models.generateContentStream({
      model: ANALYSIS_MODEL,
      contents: [{ role: 'user', parts: [{ text: feedbackPrompt }] }],
      config: {
        temperature: 0.3, // Lower temperature for consistent analysis
        maxOutputTokens: 1500,
      },
    });

    let rawAnalysis = '';
    for await (const chunk of response.stream) {
      rawAnalysis += chunk.text();
    }

    // Parse the JSON response
    const baseFeedback = JSON.parse(rawAnalysis) as SessionFeedbackData;

    // Add voice analysis if available
    let voiceAnalysis = undefined;
    if (includeVoiceAnalysis) {
      voiceAnalysis = await analyzeSessionVoiceMetrics(history);
    }

    return {
      ...baseFeedback,
      sessionId,
      speechAnalysis: voiceAnalysis,
      audioInsights: includeVoiceAnalysis ? generateAudioInsights(history) : undefined,
    };

  } catch (error) {
    console.error('üö® Session feedback generation error:', error);
    
    // Return enhanced fallback feedback
    return {
      sessionId,
      overallScore: 75,
      strengths: [
        "Engaged throughout the conversation",
        "Provided relevant examples",
        "Professional communication style"
      ],
      areasForImprovement: [
        "Provide more specific technical details",
        "Structure responses with clear examples",
        "Ask clarifying questions when appropriate"
      ],
      recommendations: [
        "Practice the STAR method for behavioral questions",
        "Research company-specific technologies and methodologies",
        "Work on confident delivery and clear communication"
      ],
      detailedAnalysis: "Your interview shows good engagement and professional communication. Focus on providing more structured responses with specific examples to strengthen your impact.",
      skillAssessment: {
        "Communication": 80,
        "Technical Knowledge": 75,
        "Problem Solving": 70,
        "Leadership": 65,
        "Adaptability": 78,
        "Teamwork": 82
      },
      speechAnalysis: includeVoiceAnalysis ? {
        overallDelivery: 70,
        pronunciationScore: 80,
        paceAndFlow: 75,
        confidenceLevel: 65,
        recommendations: [
          "Practice speaking with more confidence",
          "Work on reducing filler words",
          "Focus on consistent pacing"
        ]
      } : undefined
    };
  }
}

// üé§ SESSION VOICE METRICS ANALYSIS

async function analyzeSessionVoiceMetrics(history: MvpSessionTurn[]): Promise<{
  overallDelivery: number;
  pronunciationScore: number;
  paceAndFlow: number;
  confidenceLevel: number;
  recommendations: string[];
}> {
  // Extract voice analysis from individual turns (if available)
  const voiceAnalyses = history
    .filter(turn => turn.role === 'user')
    .map(turn => (turn as any).voiceAnalysis) // Type assertion for voice data
    .filter(Boolean);

  if (voiceAnalyses.length === 0) {
    return {
      overallDelivery: 70,
      pronunciationScore: 80,
      paceAndFlow: 75,
      confidenceLevel: 65,
      recommendations: [
        "Voice analysis not available for this session",
        "Consider using voice mode for detailed speech feedback",
        "Practice speaking clearly and confidently"
      ]
    };
  }

  // Aggregate voice metrics across the session
  const avgConfidence = voiceAnalyses.reduce((sum, analysis) => 
    sum + analysis.voiceMetrics.voiceConfidence, 0) / voiceAnalyses.length;
  
  const totalFillerWords = voiceAnalyses.reduce((sum, analysis) => 
    sum + analysis.voiceMetrics.filler_words, 0);

  const avgPace = voiceAnalyses.reduce((sum, analysis) => 
    sum + analysis.voiceMetrics.avgPace, 0) / voiceAnalyses.length;

  // Generate recommendations based on patterns
  const recommendations: string[] = [];
  
  if (avgConfidence < 70) {
    recommendations.push("Work on speaking with more confidence and conviction");
  }
  
  if (totalFillerWords > 5) {
    recommendations.push("Practice reducing filler words like 'um' and 'uh'");
  }
  
  if (avgPace < 120 || avgPace > 180) {
    recommendations.push("Practice maintaining a steady, clear speaking pace");
  }

  return {
    overallDelivery: Math.min(100, Math.max(0, avgConfidence)),
    pronunciationScore: 85, // Placeholder - would need pronunciation analysis
    paceAndFlow: Math.min(100, Math.max(0, 100 - Math.abs(avgPace - 150) / 2)),
    confidenceLevel: avgConfidence,
    recommendations: recommendations.length > 0 ? recommendations : [
      "Good overall voice delivery",
      "Continue practicing to maintain consistency"
    ]
  };
}

// üîç AUDIO INSIGHTS GENERATION

function generateAudioInsights(history: MvpSessionTurn[]): string[] {
  const insights: string[] = [];
  
  const userTurns = history.filter(turn => turn.role === 'user');
  
  if (userTurns.length > 0) {
    insights.push(`Participated in ${userTurns.length} voice exchanges`);
    insights.push("Voice responses were captured for detailed analysis");
    insights.push("Speech patterns and delivery were assessed for professional communication");
  } else {
    insights.push("No voice data available for this session");
    insights.push("Consider using voice mode for speech analysis feedback");
  }

  return insights;
}

// üõ†Ô∏è HELPER FUNCTIONS

function extractInsightsFromResponse(userResponse: string): string[] {
  const insights: string[] = [];
  const response = userResponse.toLowerCase();

  if (response.includes('challenge') || response.includes('difficult')) {
    insights.push("Shows willingness to tackle challenging problems");
  }
  
  if (response.includes('team') || response.includes('collaborate')) {
    insights.push("Demonstrates collaborative approach");
  }
  
  if (response.includes('learn') || response.includes('research')) {
    insights.push("Shows continuous learning mindset");
  }

  if (insights.length === 0) {
    insights.push("Provides relevant examples from experience");
    insights.push("Shows practical understanding of the topic");
  }

  return insights.slice(0, 3);
}

// üîÑ EXPORT THE ENHANCED FUNCTIONS

export {
  continueConversationWithVoice,
  generateEnhancedSessionFeedback,
  type VoiceConversationResponse,
  type EnhancedSessionFeedback,
  type AudioAnalysisResult,
  type VoiceMetrics
}; 